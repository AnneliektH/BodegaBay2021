{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45fdd2d",
   "metadata": {},
   "source": [
    "# Getting viral sequences from metagenomic sequencing data\n",
    "Anneliek ter Horst, May 2022\n",
    "\n",
    "We are starting with metagenomic sequencing data from Bodega Bay, CA. Part of this is done using Snakemake\n",
    "\n",
    "1. Clean sequencing reads: remove adapter sequences using trimmomatic\n",
    "2. Clean sequencing reads: remove PhiX spike using bbduk\n",
    "3. Assembly into contigs using MEGAHIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa2549",
   "metadata": {},
   "source": [
    "## Trimmomatic\n",
    "- http://www.usadellab.org/cms/?page=trimmomatic\n",
    "- Bolger, A. M., Lohse, M., & Usadel, B. (2014). Trimmomatic: A flexible trimmer for Illumina Sequence Data. Bioinformatics, btu170."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmomatic snakefile\n",
    "# run trimmomatic on raw reads\n",
    "rule trimmomatic:\n",
    "    input:\n",
    "        forward_raw_1 = \"reads/raw/{sample}_L001_R1_001.fastq.gz\",\n",
    "        reverse_raw_1 = \"reads/raw/{sample}_L001_R2_001.fastq.gz\",\n",
    "    output:\n",
    "        forward_paired_1 = \"reads/trimmomatic/{sample}_L001_R1_paired.fq.gz\",\n",
    "        forward_unpaired_1 = \"reads/trimmomatic/unpaired/{sample}_L001_R1_unpaired.fq.gz\",\n",
    "        reverse_paired_1 = \"reads/trimmomatic/{sample}_L001_R2_paired.fq.gz\",\n",
    "        reverse_unpaired_1 = \"reads/trimmomatic/unpaired/{sample}_L001_R2_unpaired.fq.gz\",\n",
    "        check = \"reads/trimmomatic/out/{sample}_trim_done.txt\"\n",
    "        \n",
    "    message: \"Trimming Illumina adapters from {input.forward_raw_1} and {input.reverse_raw_1}\"\n",
    "    shell:'''\n",
    "        module load java\n",
    "        java -jar /programs/Trimmomatic-0.39/trimmomatic-0.39.jar PE -threads 8 -phred33 \\\n",
    "        {input.forward_raw_3} {input.reverse_raw_3} \\\n",
    "        {output.forward_paired_3} {output.forward_unpaired_3} \\\n",
    "        {output.reverse_paired_3} {output.reverse_unpaired_3} \\\n",
    "        ILLUMINACLIP:/programs/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10 \\\n",
    "        SLIDINGWINDOW:4:30 MINLEN:50 && touch {output.check}\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90a493",
   "metadata": {},
   "source": [
    "# Removing phiX spike using bbmap\n",
    "- https://jgi.doe.gov/data-and-tools/software-tools/bbtools/\n",
    "- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0185056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e396f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove phiX spike from trimmed reads that are paired and unpaired\n",
    "rule remove_phix:\n",
    "    input:\n",
    "        check = \"reads/trimmomatic/out/{sample}_trim_done.txt\",\n",
    "        forward_trim = \"reads/trimmomatic/{sample}_R1_paired.fq.gz\",\n",
    "        reverse_trim = \"reads/trimmomatic/{sample}_R2_paired.fq.gz\",\n",
    "        unpaired_1 = \"reads/trimmomatic/unpaired/{sample}_R1_unpaired.fq.gz\",\n",
    "        unpaired_2 = \"reads/trimmomatic/unpaired/{sample}_R2_unpaired.fq.gz\",\n",
    "        reference = \"/programs/bbduk/phix174_ill.ref.fa\"\n",
    "    output:\n",
    "        forward_rm = \"reads/rmphix/{sample}_R1_rmphix.fq.gz\",\n",
    "        reverse_rm = \"reads/rmphix/{sample}_R2_rmphix.fq.gz\",\n",
    "        unpaired_1 = \"reads/rmphix/unpaired/{sample}_R1_unpaired.fq.gz\",\n",
    "        unpaired_2 = \"reads/rmphix/unpaired/{sample}_R2_unpaired.fq.gz\",\n",
    "        out_stats_un_1 = \"reads/rmphix/unpaired/stats/{sample}_R1_stats.txt\",\n",
    "        out_stats_un_2 = \"reads/rmphix/unpaired/stats/{sample}_R2_stats.txt\",\n",
    "        out_stats = \"reads/rmphix/stats/{sample}_stats.txt\",\n",
    "        check = \"reads/rmphix/out/{sample}_rmphix_done.txt\"\n",
    "\n",
    "    # print a message on what its doing\n",
    "    message: \"removing PhiX spike and host reads from {input.forward_trim} and {input.reverse_trim}\"\n",
    "    shell:'''\n",
    "    module load java\n",
    "    module load bbmap\n",
    "\n",
    "    bbduk.sh in1={input.forward_trim} in2={input.reverse_trim} threads=8 \\\n",
    "    out1={output.forward_rm} out2={output.reverse_rm} stats={output.out_stats} \\\n",
    "    ref={input.reference} k=31 \\\n",
    "    hdist=1 && bbduk.sh in={input.unpaired_1} out={output.unpaired_1} threads=8 \\\n",
    "    stats={output.out_stats_un_1} ref={input.reference} k=31 \\\n",
    "    hdist=1 && bbduk.sh in={input.unpaired_2} out={output.unpaired_2} threads=8 \\\n",
    "    stats={output.out_stats_un_2} ref={input.reference} k=31 \\\n",
    "    hdist=1 && touch {output.check}\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfd34d",
   "metadata": {},
   "source": [
    "# Assembly of the clean reads using MEGAHIT\n",
    "- https://github.com/voutcn/megahit\n",
    "- https://academic.oup.com/bioinformatics/article/31/10/1674/177884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6025f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# megahit needs a temp folder, as snakemake doesn't take folders as an output (thanks Taylor)\n",
    "\n",
    "rule PE_assembly:\n",
    "    input: \n",
    "        forward_read = \"reads/trimmomatic/{sample}_L002_R1_paired.fq.gz\", \n",
    "        reverse_read = \"reads/trimmomatic/{sample}_L002_R2_paired.fq.gz\",\n",
    "        unpaired_1 = \"reads/trimmomatic/unpaired/{sample}_L002_R1_unpaired.fq.gz\",\n",
    "        unpaired_2 = \"reads/trimmomatic/unpaired/{sample}_L003_R1_unpaired.fq.gz\", \n",
    "    output:\n",
    "        check = \"assemblies/out/{sample}_assem_done.txt\",\n",
    "        out_contig = \"assemblies/megahit_final/{sample}.contigs.fa\" \n",
    "    params:\n",
    "        output_folder = \"assemblies/megahit_final\",\n",
    "        output_temp = \"assemblies/megahit_temp\"\n",
    "    message: \"paired end assembly on {input.forward_read}\"\n",
    "    shell:'''\n",
    "    mkdir -p  assemblies/megahit_temp/\n",
    "    module load megahit\n",
    "    \n",
    "    # megahit does not allow force overwrite, so each assembly needs to take place in it's own directory. \n",
    "    megahit -1 {input.forward_read} -2 {input.reverse_read} \\\n",
    "    -r {input.unpaired_1},{input.unpaired_2} \\\n",
    "    -t 16 --continue --k-min 27 --min-contig-len 1000 --presets meta-large -m 0.095 \\\n",
    "    --out-dir {params.output_temp}/{wildcards.sample} \\\n",
    "    --out-prefix {wildcards.sample} && \\\n",
    "    mv {params.output_temp}/{wildcards.sample}/{wildcards.sample}.contigs.fa \\\n",
    "    {params.output_folder} && touch {output.check}\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90b128",
   "metadata": {},
   "source": [
    "## Renaming contigs and split on lenght\n",
    "- MEGAHIT outputs contigs with spaces in filenames, don't like that\n",
    "- Only keep contigs > 10kb for predicting viruses\n",
    "- Using bbmap package\n",
    "- https://jgi.doe.gov/data-and-tools/software-tools/bbtools/\n",
    "- https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0185056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ab900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename contigs and  split off contigs >10kb\n",
    "rule rename_contigs:\n",
    "    input:\n",
    "        contigs = \"assemblies/megahit_final/{sample}.contigs.fa\",\n",
    "        check = \"assemblies/out/{sample}_assem_done.txt\"\n",
    "    output:\n",
    "        contigs_renamed = \"outputs/megahit_final/{sample}.contigs_renamed.fa\",\n",
    "        assembly_stats = \"outputs/megahit_final/stats_assembly/{sample}.megahit.stats.txt\",\n",
    "        contigs_10kb = \"outputs/megahit_final/{sample}.contigs_10kb.fa\",\n",
    "        check = \"assemblies/out/{sample}_reformat_done.txt\"\n",
    "    # print a message on what its doing\n",
    "    message: \"renaming contigs of sample {input.contigs}\"\n",
    "    shell:'''\n",
    "    module load bbmap\n",
    "    rename.sh in={input.contigs} out={output.contigs_renamed} prefix={wildcards.sample} && \\\n",
    "    reformat.sh in={output.contigs_renamed} out={output.contigs_10kb} minlength=10000 &&\n",
    "    stats.sh in={output.contigs_renamed} gc={output.assembly_stats} gcformat=3 && touch {output.check}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87275f5",
   "metadata": {},
   "source": [
    "## Use VIBRANT to predict viral contigs from all contigs\n",
    "- https://github.com/AnantharamanLab/VIBRANT\n",
    "- https://pubmed.ncbi.nlm.nih.gov/32522236/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing with a temp folder for snakemake as with MEGAHIT\n",
    "# run vibrant\n",
    "rule vibrant:\n",
    "    input:\n",
    "        contigs = \"outputs/megahit_final/{sample}.contigs_10kb.fa\",\n",
    "        check = \"assemblies/out/{sample}_reformat_done.txt\"\n",
    "    output:\n",
    "        vibrant_contig = \"outputs/vibrant_final/{sample}.contigs_10kb.phages_combined.fna\",\n",
    "        check = \"outputs/out/{sample}_vibrant_done.txt\"\n",
    "    params:\n",
    "        output_folder = \"outputs/vibrant_final\",\n",
    "        output_temp = \"outputs/vibrant_temp\"\n",
    "    # print a message on what its doing\n",
    "    message: \"running vibrant on {input.contigs}\"\n",
    "    shell:'''\n",
    "    mkdir -p  outputs/vibrant_temp/\n",
    "    \n",
    "    set +u\n",
    "    source ~/.bashrc\n",
    "    conda activate vibrant\n",
    "    set -u\n",
    "    \n",
    "    VIBRANT_run.py -i {input.contigs} -folder {params.output_temp}/{wildcards.sample} \\\n",
    "    -t 8 -l 10000 -o 4 -virome && \\\n",
    "    mv {params.output_temp}/{wildcards.sample}/VIBRANT_{wildcards.sample}.contigs_10kb/VIBRANT_phages_{wildcards.sample}.contigs_10kb/{wildcards.sample}.contigs_10kb.phages_combined.fna \\\n",
    "    {params.output_folder} && touch {output.check} \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4be746",
   "metadata": {},
   "source": [
    "## Use dRep to dereplicate viral contigs. \n",
    "- https://drep.readthedocs.io/en/latest/\n",
    "- https://www.nature.com/ismej/articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run drep on all viral contigs predicted by vibrant (n=17703) + all found by PIGEON (581)\n",
    "\n",
    "\n",
    "# split contigs into individual fastas (dRep accepts individual contigs only)\n",
    "mkdir contigs\n",
    "cd ./contigs\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' ../*.fa \n",
    "\n",
    "\n",
    "# Activate dRep conda instance\n",
    "source ~/.bashrc\n",
    "conda activate drep\n",
    "\n",
    "# Load other programs that dRep needs\n",
    "module load mummer\n",
    "module load mash\n",
    "module load bowtie2\n",
    "\n",
    "# Run dRep at 95% ANI over 85% of length of longest contigs\n",
    "dRep dereplicate ./drep --S_algorithm ANImf --ignoreGenomeQuality -l 10000 -sa 0.95 -nc 0.85 \\\n",
    "-g ./contigs/*.fa -p 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13cd6e",
   "metadata": {},
   "source": [
    "## Use Bowtie2 to map back reads to viral contigs\n",
    "- Make a bowtie2 index\n",
    "- map reads to index using bowtie2\n",
    "- http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\n",
    "- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322381/\n",
    "- Convert samfiles to bamfiles using samtools\n",
    "- index bamfiles using samtools\n",
    "- http://www.htslib.org/\n",
    "- https://academic.oup.com/bioinformatics/article/25/16/2078/204688\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First build bowtie2 index from viral contigs (n=14512, dereplicated)\n",
    "\n",
    "# concat all dereplicated contigs\n",
    "cat ./drep/dereplicated_genomes/*.fa >> ./220315_wetland_votus_drep_animf.fa\n",
    "\n",
    "# build index file for mapping\n",
    "module load bowtie2\n",
    "bowtie2-build 220315_wetland_votus_drep_animf.fa ./bowtie2/220315_wetland_votus_drep -p 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now map the reads to this bowtie2 index\n",
    "# Then sam -> bam\n",
    "# Then index bamfile\n",
    "# Run bowtie\n",
    "rule bowtie:\n",
    "    input:\n",
    "        index = 'outputs/bowtie2/220315_wetland_votus_drep',\n",
    "        fw_read_1 = \"reads/trimmomatic/{sample}_L002_R1_paired.fq.gz\", \n",
    "        rv_read_1 = \"reads/trimmomatic/{sample}_L002_R2_paired.fq.gz\",\n",
    "    output:\n",
    "        samfile = \"outputs/mapping/{sample}.sam\",\n",
    "        bamfile = \"outputs/mapping/{sample}.bam\",\n",
    "        check = \"outputs/mapping/out/{sample}_done.txt\"\n",
    "\n",
    "        \n",
    "    # print a message on what its doing\n",
    "    message: \"running bowtie on {input.fw_read_1}\"\n",
    "    shell:'''\n",
    "    module load bowtie2\n",
    "    module load samtools\n",
    "    \n",
    "    bowtie2 --threads 12 -x ./outputs/bowtie2/220315_wetland_votus_drep \\\n",
    "    -1 {input.fw_read_1},{input.fw_read_2}\\\n",
    "    -2 {input.rv_read_1},{input.rv_read_2} \\\n",
    "    -S {output.samfile} --no-unal --sensitive && \\\n",
    "    samtools view -@ 12 -F 4 -bS {output.samfile} | samtools sort > {output.bamfile} && \\\n",
    "    samtools index {output.bamfile} && touch {output.check}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d89b0f",
   "metadata": {},
   "source": [
    "## Use CoverM to make a coverage table\n",
    "- https://github.com/wwood/CoverM\n",
    "- https://wwood.github.io/CoverM/coverm-genome.html#author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate coverM instance in conda\n",
    "source ~/.bashrc\n",
    "conda activate coverm\n",
    "\n",
    "# make a coverage table, where the min amount of the contig that has to be covered is 75%\n",
    "coverm contig -m mean --min-covered-fraction 0.75 -b *.bam > 211220_coverM_wetland_mean_75.tsv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1b9c4",
   "metadata": {},
   "source": [
    "## Use Sortmerna to extract 16S reads from the metagenomes\n",
    "- https://bioinfo.lifl.fr/RNA/sortmerna/\n",
    "- https://academic.oup.com/bioinformatics/article/28/24/3211/246053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other programs that sortmerna needs\n",
    "module load bbmap\n",
    "module load samtools\n",
    "\n",
    "# activate sortmerna environment\n",
    "source ~/.bashrc\n",
    "conda activate sortmerna\n",
    "\n",
    "# do this for each fastq file (paired reads)\n",
    "for f in *_R1*.gz\n",
    "do \n",
    "sortmerna --ref ./16S_db/silva-bac-16s-database-id85.fasta \\\n",
    "--ref ./16S_db/silva-arc-16s-database-id95.fasta \\\n",
    "--reads $1 \\\n",
    "--reads ${1%R1*}R2.fastq.gz \\\n",
    "--workdir ./sortmerna/${1%_R1*} \\\n",
    "--fastx \\\n",
    "--aligned ./sortmerna/sort_results/${1%R1*}rrna \\\n",
    "-v --threads 24 --paired_out\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76809a17",
   "metadata": {},
   "source": [
    "## Use RDP tools to classify the found 16S rRNA reads\n",
    "- http://rdp.cme.msu.edu/\n",
    "- https://academic.oup.com/nar/article/42/D1/D633/1063201?login=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate conda env\n",
    "source ~/.bashrc\n",
    "conda activate rdptools\n",
    "\n",
    "# Use the classifier command\n",
    "for f in *.fq.gz \n",
    "do \n",
    "echo $f\n",
    "java -jar ./miniconda3/envs/rdptools/bin/classifier.jar classify \\\n",
    "-g 16srrna \\\n",
    "-f fixrank \\\n",
    "-b ${f%%.fq.gz*}.bootstrap \\\n",
    "-h ${f%%.fq.gz*}.hier.tsv \\\n",
    "-o ${f%%.fq.gz*}.class.tsv \\\n",
    "./$f \n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2d457",
   "metadata": {},
   "source": [
    "## Use CRASS to predict crispr - spacer regions in both host and virus \n",
    "- https://ctskennerton.github.io/crass/\n",
    "- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3664793/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate crass environment\n",
    "source ~/.bashrc\n",
    "conda activate crass\n",
    "\n",
    "# Do for each paired fastq file\n",
    "for f in *R1*.gz \n",
    "do\n",
    "crass $f ${f%%_R1*}_R2.fastq.gz -o ./crass/${f%%_R1*} -l 4\n",
    "done\n",
    "\n",
    "# Also look for spacers in the 2019 reads\n",
    "\n",
    "# extract spacer and repeat sequences\n",
    "for f in */crass.crispr; do\n",
    "echo $f\n",
    "crisprtools extract -o ${f%%crass.crispr*}spacers -s $f -x \n",
    "done\n",
    "\n",
    "\n",
    "# all the repeats and spacers need a prefix with the sample name, cause there is multiple spacers/repeats with the same name\n",
    "module load bbmap\n",
    "for f in *\n",
    "do\n",
    "echo $f\n",
    "cd $f \n",
    "# concatenate \n",
    "cat ./spacers/*.fa > all_spacers_crass.fa\n",
    "# rename\n",
    "rename.sh in=all_spacers_crass.fa out=all_spacers_crass_rename.fa prefix=$f addprefix=t\n",
    "cd ..\n",
    "done\n",
    "\n",
    "\n",
    "# concatenate all spacers\n",
    "cat */all_spacers_crass_rename.fa > all_spacers_crass.fa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use blast to match to the spacers:\n",
    "# First make a fasta file of all 2019 and 2021 sequences\n",
    "\n",
    "cat 220908_2019_BB_vOTUs.fa 220908_all_2019_2021_BB_votus.fa >> 220908_all_2019_2021_BB_votus.fa\n",
    "\n",
    "# remove the duplicates, aka the ones that were in both files\n",
    "source ~/.bashrc\n",
    "conda activate seqkit\n",
    "\n",
    "seqkit rmdup --quiet -n 220908_all_2019_2021_BB_votus.fa > 220908_all_2019_2021_BB_votus.fa.dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make a blastdb of all vOTU sequences (including all 2019 BB vOTUs.)\n",
    "makeblastdb -in 220908_all_2019_2021_BB_votus.fa.dedupe -dbtype nucl -out vOTU_BB_db\n",
    "\n",
    "# blast back blastn -eval 1e-10 and 100% nucl ident\n",
    "\n",
    "# WE BLAST THE SPACER SEQUENCES TO THE VOTUS\n",
    "blastn -task blastn-short \\\n",
    "-query all_spacers_crass.fa \\\n",
    "-db ../outputs/vOTU_BB_db -evalue 1e-10 -perc_identity 95 -outfmt 6 \\\n",
    "-out spacers_vOTUs_crass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d172a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract spacer and repeat sequences\n",
    "for f in BB_*/crass.crispr; do\n",
    "echo $f\n",
    "crisprtools extract -o ${f%%crass.crispr*}spacers -s $f -x \n",
    "done\n",
    "\n",
    "\n",
    "# all the repeats and spacers need a prefix with the sample name, cause there is multiple spacers/repeats with the same name\n",
    "module load bbmap\n",
    "for f in *\n",
    "do\n",
    "echo $f\n",
    "cd $f \n",
    "# concatenate \n",
    "cat ./spacers/*.fa > all_spacers_crass.fa\n",
    "# rename\n",
    "rename.sh in=all_spacers_crass.fa out=all_spacers_crass_rename.fa prefix=$f addprefix=t\n",
    "cd ..\n",
    "done\n",
    "\n",
    "\n",
    "# concatenate all spacers\n",
    "cat */all_spacers_crass_rename.fa > all_spacers_crass.fa\n",
    "\n",
    "# dediplicate by sequeonce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -D /home/csantosm/wetup/scripts/\n",
    "#SBATCH --job-name=iphop_split\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 1:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate IPHOP\n",
    "\n",
    "set=${1}\n",
    "\n",
    "full_db=/home/amhorst/2022_wetlands/outputs/220518_drep_all_vOTUs_recov_BB.fa\n",
    "split_db=/home/amhorst/2022_wetlands/outputs/iphop/split_db\n",
    "\n",
    "iphop split --input_file ${full_db} --split_dir ${split_db}\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run iphop for host predictions\n",
    "## The db needs to be chmodded \n",
    "\n",
    "for f in *.fna\n",
    "do\n",
    "echo $f\n",
    "sbatch iphop.sh $f\n",
    "done\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=iphop_predict\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH --mem=128GB\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate IPHOP\n",
    "\n",
    "iphop predict \\\n",
    "--fa_file $1 \\\n",
    "--out_dir ../results/${1%.fna*} \\\n",
    "--db_dir /home/csantosm/databases/IPHOP_db/Sept_2021_pub \\\n",
    "--num_threads 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try vamb for binning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=vamb\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 168:00:00\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "\n",
    "# loading modules\n",
    "source /home/csantosm/initconda\n",
    "conda activate VAMB\n",
    "\n",
    "vamb --outdir ./vamb --fasta ./assemblies/coverage/all_bulk_contigs_renamed_cluster.fa \\\n",
    "--jgi /home/amhorst/2022_wetlands/assemblies/coverage/mapping/metabat_depth.txt --minfasta 50000\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
